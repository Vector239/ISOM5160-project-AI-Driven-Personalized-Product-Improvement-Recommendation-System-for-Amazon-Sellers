# File: negative_review_analysis.py
# Author: LIAO, Jingyu
# Student ID: 21262106
# Email: jliaoae@connect.ust.hk
# Date: 2025-10-03
# Description: Analyze negative review reasons based on comment keywords


### 代码功能说明

# 该代码专为项目设计，基于评论关键词实现差评原因分析，与现有代码库深度兼容：


### Code Function Description
# This code is specifically designed for the project, enabling the analysis of negative review reasons based on comment keywords, and is deeply compatible with the existing codebase:
    #1. **Data Compatibility**:```
# - Directly read the `amazon_food_reviews_cleaned.csv` data generated by `data_cleaning.py````
   # - Use the same text preprocessing logic as the data cleaning module (stopword list, lemmatization)
   #- Filter genuine negative reviews using dual criteria of sentiment analysis scores (`sentiment_score`) and ratings (`Score`)
#2. **Core Analysis Process**:
   #- **Negative Review Identification**: Dual filtering with a rating ≤2 and an emotion score <0.4 ensures the analysis targets genuine negative reviews
   #- **Keyword Processing**: Perform lemmatization and stopword filtering on negative review keywords to enhance analysis accuracy
   #- **Reason Categorization**: Map high-frequency keywords to 8 predefined categories (such as taste, quality, expiration, and other common food-related issues)
   #```- **Visual Output**: Generate keyword frequency charts and cause classification pie charts to visually present the analysis results```
   #```# - **Product-Level Insights**: For products with the most negative reviews, extract their primary negative keywords to provide sellers with specific improvement directions.```
#3. **Usage**:
   #1. Run `data_cleaning.py` first to generate the cleaned data```
   #2. Execute this file: `python negative_review_analysis.py`
   #The results will be saved to the `datasets` directory, including:
      # - Categorized Negative Keywords Table (`negative_keywords_categorized.csv`)```
      # - Product Negative Insights Table (`top_negative_products.csv`)```
      #- Two visualizations (keyword frequency chart and cause classification chart)
# The code can easily add new analysis categories by extending the `category_mapping` dictionary, accommodating more granular business needs. {insert_element_0_}```


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import os
import nltk
from typing import Tuple, List, Dict

# Download required NLTK resources (consistent with data_cleaning.py)
nltk.download('wordnet', quiet=True)
nltk.download('stopwords', quiet=True)

# Initialize text processing tools (aligned with data_cleaning.py)
lemmatizer = WordNetLemmatizer()
additional_stopwords = {'product', 'item', 'food', 'amazon', 'buy', 'purchased', 'purchase', 'review'}
stop_words = set(stopwords.words('english')).union(additional_stopwords)


def load_cleaned_reviews(file_path: str = 'datasets/amazon_food_reviews_cleaned.csv') -> pd.DataFrame:
    """
    Load cleaned review data processed by data_cleaning.py
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Cleaned data not found. Run data_cleaning.py first to generate {file_path}")

    df = pd.read_csv(file_path)

    # Convert keyword strings back to lists (handle CSV serialization)
    df['keywords'] = df['keywords'].apply(
        lambda x: eval(x) if pd.notna(x) and x != '[]' else []
    )

    # Ensure necessary columns exist
    required_columns = ['ProductId', 'Score', 'keywords', 'sentiment_score', 'Text_cleaned']
    if not set(required_columns).issubset(df.columns):
        missing = set(required_columns) - set(df.columns)
        raise ValueError(f"Cleaned data missing required columns: {missing}")

    return df


def identify_negative_reviews(df: pd.DataFrame, score_threshold: int = 2) -> pd.DataFrame:
    """py
    Filter reviews with low scores (negative reviews) using dual criteria:
    - Score <= threshold
    - sentiment_score < 0.4 (from data_cleaning.py's sentiment analysis)
    """
    negative_mask = (df['Score'] <= score_threshold) & (df['sentiment_score'] < 0.4)
    negative_reviews = df[negative_mask].copy()

    print(f"Identified {len(negative_reviews)} negative reviews "
          f"(Score ≤{score_threshold} and sentiment score <0.4)")

    return negative_reviews


def process_negative_keywords(negative_reviews: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    """
    Process keywords from negative reviews: lemmatization and stopword removal
    Consistent with text processing in data_cleaning.py
    """

    def clean_keywords(keywords: List[str]) -> List[str]:
        """Clean individual keyword list"""
        cleaned = []
        for kw in keywords:
            # Lemmatize to get root form
            lemma = lemmatizer.lemmatize(kw.lower())
            # Filter stopwords and short tokens
            if lemma not in stop_words and len(lemma) > 2:
                cleaned.append(lemma)
        return cleaned

    # Apply cleaning to all negative reviews
    negative_reviews['cleaned_keywords'] = negative_reviews['keywords'].apply(clean_keywords)

    # Collect all keywords for frequency analysis
    all_negative_keywords = []
    for kw_list in negative_reviews['cleaned_keywords']:
        all_negative_keywords.extend(kw_list)

    return negative_reviews, all_negative_keywords


def analyze_keyword_patterns(keywords: List[str], top_n: int = 30) -> pd.DataFrame:
    """
    Analyze frequency and patterns of negative keywords
    """
    keyword_counts = Counter(keywords)
    top_keywords = keyword_counts.most_common(top_n)
    return pd.DataFrame(top_keywords, columns=['keyword', 'frequency'])


def categorize_negative_reasons(top_keywords: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """
    Map top keywords to predefined negative reason categories
    """
    # Category mapping (expanded with common food product issues)
    category_mapping = {
        'taste': ['taste', 'flavor', 'flavour', 'bitter', 'sweet', 'sour', 'salty', 'bland'],
        'quality': ['quality', 'bad', 'poor', 'low', 'cheap', 'awful', 'terrible'],
        'expired': ['expired', 'expiry', 'date', 'stale', 'rotten', 'spoiled', 'moldy'],
        'packaging': ['package', 'packaging', 'box', 'broken', 'damaged', 'seal', 'leak', 'crushed'],
        'price': ['price', 'expensive', 'overpriced', 'cost', 'worthless'],
        'delivery': ['late', 'delivery', 'ship', 'arrive', 'slow'],
        'quantity': ['small', 'large', 'size', 'quantity', 'amount', 'little', 'lot'],
        'smell': ['smell', 'odor', 'stink', 'foul', 'sour']
    }

    # Reverse mapping for keyword lookup
    keyword_to_category = {}
    for category, terms in category_mapping.items():
        for term in terms:
            keyword_to_category[term] = category

    # Assign categories to top keywords
    top_keywords['category'] = top_keywords['keyword'].apply(
        lambda x: keyword_to_category.get(x, 'other')
    )

    return top_keywords, category_mapping


def visualize_negative_patterns(
        categorized_keywords: pd.DataFrame,
        output_dir: str = 'datasets'
):
    """
    Generate visualizations for negative review analysis
    """
    os.makedirs(output_dir, exist_ok=True)

    # 1. Top keywords visualization
    plt.figure(figsize=(12, 8))
    ax1 = sns.barplot(
        data=categorized_keywords,
        x='frequency',
        y='keyword',
        hue='category',
        palette='Reds_d'
    )
    plt.title('Top Keywords in Negative Reviews', fontsize=14, fontweight='bold')
    plt.xlabel('Frequency')
    plt.ylabel('Keyword')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/negative_keywords_top.png", dpi=300)
    plt.close()

    # 2. Category distribution
    category_summary = categorized_keywords.groupby('category')['frequency'].sum().sort_values(ascending=False)

    plt.figure(figsize=(10, 6))
    ax = category_summary.plot(
        kind='pie',
        autopct='%1.1f%%',
        colors=sns.color_palette('Reds', n_colors=len(category_summary))
    )
    plt.title('Distribution of Negative Review Reasons', fontsize=14, fontweight='bold')
    plt.ylabel('')  # Remove redundant y-label
    plt.tight_layout()
    plt.savefig(f"{output_dir}/negative_reason_categories.png", dpi=300)
    plt.close()

    print(f"Visualizations saved to {output_dir}")
    return ax, ax1


def generate_product_specific_insights(
        negative_reviews: pd.DataFrame,
        top_n_products: int = 10
) -> pd.DataFrame:
    """
    Generate product-specific negative reason insights
    """
    # Group by product and aggregate negative keywords
    product_issues = negative_reviews.groupby('ProductId').agg({
        'cleaned_keywords': lambda x: [kw for sublist in x for kw in sublist],
        'Score': 'count'
    }).rename(columns={'Score': 'negative_review_count'})

    # Get top keywords per product
    product_issues['top_issues'] = product_issues['cleaned_keywords'].apply(
        lambda x: [kw for kw, _ in Counter(x).most_common(5)]
    )

    # Sort by number of negative reviews
    return product_issues.sort_values('negative_review_count', ascending=False).head(top_n_products)


def save_analysis_results(
        categorized_keywords: pd.DataFrame,
        product_insights: pd.DataFrame,
        output_dir: str = 'datasets'
) -> None:
    """
    Save analysis results to CSV files
    """
    os.makedirs(output_dir, exist_ok=True)
    categorized_keywords.to_csv(f"{output_dir}/negative_keywords_categorized.csv", index=False)
    product_insights.to_csv(f"{output_dir}/top_negative_products.csv")
    print(f"Analysis results saved to {output_dir}")


def main():
    # Step 1: Load cleaned data (from data_cleaning.py)
    print("Loading cleaned review data...")
    review_data = load_cleaned_reviews()

    # Step 2: Identify negative reviews using dual criteria
    print("Identifying negative reviews...")
    negative_reviews = identify_negative_reviews(review_data)

    # Step 3: Process and analyze keywords
    print("Analyzing negative keywords...")
    processed_reviews, all_neg_kw = process_negative_keywords(negative_reviews)
    top_keywords = analyze_keyword_patterns(all_neg_kw)

    # Step 4: Categorize negative reasons
    print("Categorizing negative reasons...")
    categorized_kw, categories = categorize_negative_reasons(top_keywords)

    # Step 5: Generate visualizations
    ax1, ax2 = visualize_negative_patterns(categorized_kw)

    # Step 6: Generate product-specific insights
    product_insights = generate_product_specific_insights(processed_reviews)

    # Step 7: Save results
    save_analysis_results(categorized_kw, product_insights)

    # Print summary
    print("\n=== Negative Review Analysis Summary ===")
    print("Top Negative Reason Categories:")
    print(categorized_kw.groupby('category')['frequency'].sum().sort_values(ascending=False))

    return ax1, ax2


if __name__ == "__main__":
    main()
