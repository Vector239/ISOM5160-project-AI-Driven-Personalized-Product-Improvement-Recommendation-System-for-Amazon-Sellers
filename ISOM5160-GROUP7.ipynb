{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ISOM5160 Project - Group7\n",
    "\n",
    "- **Dataset:** amazon_food_reviews.csv\n",
    "- **Topic:** AI Driven Personalized Product Improvement Recommendation System for Amazon Sellers\n",
    "- **Individual Contribution:**\n",
    "\n",
    "| Name          | SID      | Contributions                                                                                                          |\n",
    "|---------------|----------|------------------------------------------------------------------------------------------------------------------------|\n",
    "| CAO, Xi       | 21271664 | Review Text Preprocessing; Sentiment Analysis; Review Keyword Extraction; Data Cleaning                                                                          |\n",
    "| LI, Heyi      | 21265689 | Anomaly Analysis of Comments and Ratings (Based on Sentiment Analysis of Comments), PPT Coordination                                                                          |\n",
    "| LIAO, Jingyu  | 21262106 | Analysis of Negative Review Reasons (Based on Review Keywords)                                                                                          |\n",
    "| LIN, Chuwei   | 21237955 | Time Trend Analysis of Rating                                                                                                          |\n",
    "| YE, Chenwei   | 21199517 | User Comment Weight Analysis (simple weighted & Bayesian smooth)                                                                                                        |\n",
    "| ZHANG, Ziyang | 21266920 | 1. Data scraping: additional amazon product info <br/>2. Analyse: Correlation Between Ratings and Product Descriptions |\n",
    "\n",
    "\n",
    "## 0. Install requirements and do initialization\n",
    "\n",
    "### 0.1 Install requirements & unzip scrapped data"
   ],
   "id": "ab62897204b4a2b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run first to install requirements\n",
    "!pip install -r requirements.txt > /dev/null\n",
    "\n",
    "# Run this to speed up the data scraping process\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"datasets/new_data.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"datasets\")  # 解压到datasets文件夹里"
   ],
   "id": "2b6626a519e68a9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0.2 Run data cleaning & load `amazon_food_reviews.csv`",
   "id": "4270c92ac1f0893d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from amazon_new_data_scraping import parallel_scrape_amazon_product_info, load_all_data_as_dataframe, \\\n",
    "    extract_comments_from_product_info\n",
    "from data_cleaning import add_sentiment_score, main as data_cleaning_main\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not os.path.exists('datasets/amazon_food_reviews_cleaned.csv'):\n",
    "    data_cleaning_main()\n",
    "\n",
    "df_amazon_food_reviews = pd.read_csv('datasets/amazon_food_reviews.csv',\n",
    "                                     converters={\"Time\": lambda x: pd.to_datetime(int(x), unit=\"s\")})\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ],
   "id": "9c55e5eeb05c86a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0.3 Scrape product info from *amazon.com* & load production info and concat to original dataset",
   "id": "c68e91a57980d803"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "InteractiveShell.ast_node_interactivity = 'last_expr'\n",
    "\n",
    "product_id_list = list(df_amazon_food_reviews.ProductId.unique())\n",
    "print(f\"Total number of products: {len(product_id_list)}\")\n",
    "print(\"Start scraping data...\")\n",
    "parallel_scrape_amazon_product_info(product_id_list, replace=False)\n",
    "print(\"Scraping finished.\")\n",
    "df_amazon_product_info = load_all_data_as_dataframe()\n",
    "print(\"Product info loaded.\")\n",
    "\n",
    "# Optional: Comment it out if not needed in 4.3.3\n",
    "df_amazon_product_info = add_sentiment_score(df_amazon_product_info, 'product_description')\n",
    "print(\"Sentiment score loaded.\")\n",
    "\n",
    "df_amazon_product_info.sample(3)\n",
    "\n",
    "# Concat new reviews to the original dataset and drop duplicates\n",
    "df_amazon_food_reviews = pd.concat([df_amazon_food_reviews, extract_comments_from_product_info(df_amazon_product_info)]\n",
    "                                   ).drop_duplicates(['Score', 'Time', 'Summary']).reset_index(drop=True)\n",
    "\n",
    "df_amazon_food_reviews.sample(3)"
   ],
   "id": "e0368738493f8dda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Negative Review Analysis",
   "id": "d9db0b34564873d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from negative_review_analysis import main as analyse_negative_review\n",
    "ax1, ax2 = analyse_negative_review()\n",
    "display(ax1.get_figure())"
   ],
   "id": "1872e8806486eb51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.Sentiment score outlier analysis\n",
    "\n",
    "### 2.1 Sentiment outliers"
   ],
   "id": "91f5046ee76812ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 确保 sentiment_score 与 Score 同区间\n",
    "df_reviews = pd.read_csv('datasets/amazon_food_reviews_cleaned.csv')\n",
    "df_reviews['sentiment_score_scaled'] = df_reviews['sentiment_score'] * 5\n",
    "\n",
    "# 计算评论情感与评分的差异\n",
    "df_reviews['sentiment_gap'] = (\n",
    "    df_reviews['Score'] - df_reviews['sentiment_score_scaled']\n",
    ")\n",
    "\n",
    "# 计算 Z-score，识别异常评论\n",
    "df_reviews['z_score'] = (\n",
    "    (df_reviews['sentiment_gap'] - df_reviews['sentiment_gap'].mean())\n",
    "    / df_reviews['sentiment_gap'].std()\n",
    ")\n",
    "df_reviews['is_outlier'] = df_reviews['z_score'].abs() > 2 # |z|>2 视为异常\n",
    "\n",
    "# 打印异常评论样例\n",
    "print(\"10 of the most mismatched reviews:\")\n",
    "display(\n",
    "    df_reviews.loc[\n",
    "        df_reviews['is_outlier'],\n",
    "        ['Id','ProductId', 'Score', 'sentiment_score_scaled', 'sentiment_gap', 'z_score','Summary','Text_cleaned']\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "# 绘制散点图：情感分 vs 评分\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(\n",
    "    df_reviews['sentiment_score_scaled'],\n",
    "    df_reviews['Score'],\n",
    "    alpha=0.3\n",
    ")\n",
    "plt.title(\"Sentiment Score (scaled) vs Rating\")\n",
    "plt.xlabel(\"Sentiment Score (scaled to 0–5)\")\n",
    "plt.ylabel(\"User Rating (0–5)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 绘制差异分布图\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist(df_reviews['sentiment_gap'], bins=50, alpha=0.7)\n",
    "plt.title(\"Distribution of Sentiment–Rating Gaps\")\n",
    "plt.xlabel(\"Gap (Score – sentiment_score_scaled)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.axvline(df_reviews['sentiment_gap'].mean(), color='red', linestyle='--', label='mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 输出统计信息\n",
    "print(\"Mean gap:\", df_reviews['sentiment_gap'].mean())\n",
    "print(\"Std gap:\", df_reviews['sentiment_gap'].std())\n",
    "print(f\"Number of outliers: {df_reviews['is_outlier'].sum()} / {len(df_reviews)}\")\n"
   ],
   "id": "b239c8145aa06cf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Comment Time Series Analysis",
   "id": "1a7e4787f5de1fb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reviews_df = df_amazon_food_reviews.copy()\n",
    "\n",
    "# Ensure datetime: handle both integer seconds and existing datetimes\n",
    "if not np.issubdtype(reviews_df['Time'].dtype, np.datetime64):\n",
    "    reviews_df['review_date'] = pd.to_datetime(reviews_df['Time'], unit='s', errors='coerce')\n",
    "else:\n",
    "    reviews_df['review_date'] = reviews_df['Time']\n",
    "\n",
    "# Extract year and month for grouping\n",
    "reviews_df['year'] = reviews_df['review_date'].dt.year\n",
    "reviews_df['month'] = reviews_df['review_date'].dt.to_period('M')"
   ],
   "id": "a821b564710c5e61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Monthly Review Volume Over Time",
   "id": "6cf4ce76c30e4aaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# Guard for empty data\n",
    "if reviews_df.empty or reviews_df['review_date'].isna().all():\n",
    "    print('No review dates available to plot.')\n",
    "else:\n",
    "    # Aggregate counts by month Period\n",
    "    monthly_series = reviews_df.groupby('month').size()\n",
    "\n",
    "    # Build continuous monthly index and fill gaps with 0\n",
    "    month_min = reviews_df['review_date'].dt.to_period('M').min()\n",
    "    month_max = reviews_df['review_date'].dt.to_period('M').max()\n",
    "    full_index = pd.period_range(start=month_min, end=month_max, freq='M')\n",
    "    monthly_series = monthly_series.reindex(full_index, fill_value=0)\n",
    "\n",
    "    # Prepare DataFrame for plotting\n",
    "    monthly_reviews = monthly_series.reset_index()\n",
    "    monthly_reviews.columns = ['month', 'review_count']\n",
    "    monthly_reviews['month_ts'] = monthly_reviews['month'].dt.to_timestamp()\n",
    "\n",
    "    plt.figure(figsize=(30,15))\n",
    "    sns.lineplot(data=monthly_reviews, x='month_ts', y='review_count', color='#1f77b4', linewidth=2)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "\n",
    "    plt.title('Monthly Review Volume Over Time', fontsize=14)\n",
    "    plt.xlabel('Month (Year-Month)')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d9f242e655de134"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Average Rating Trend Over Time",
   "id": "30f95b413fcc88be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Rating heatmap: average rating by year x month\n",
    "# Ensure required columns exist\n",
    "if 'year' not in reviews_df.columns:\n",
    "    reviews_df['year'] = reviews_df['review_date'].dt.year\n",
    "reviews_df['month_num'] = reviews_df['review_date'].dt.month\n",
    "\n",
    "# Use numeric Score\n",
    "reviews_df['Score_num'] = pd.to_numeric(reviews_df['Score'], errors='coerce')\n",
    "valid = reviews_df.dropna(subset=['review_date', 'Score_num']).copy()\n",
    "\n",
    "if valid.empty:\n",
    "    print('No valid ratings to build heatmap.')\n",
    "else:\n",
    "    rating_heatmap = (valid\n",
    "        .groupby(['year', 'month_num'])['Score_num']\n",
    "        .mean()\n",
    "        .unstack(fill_value=np.nan)\n",
    "    )\n",
    "    # Order months 1..12 and years ascending\n",
    "    month_cols = list(range(1, 13))\n",
    "    for m in month_cols:\n",
    "        if m not in rating_heatmap.columns:\n",
    "            rating_heatmap[m] = np.nan\n",
    "    rating_heatmap = rating_heatmap[month_cols].sort_index()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.heatmap(\n",
    "        rating_heatmap,\n",
    "        cmap='RdYlGn',\n",
    "        vmin=1, vmax=5,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'label': 'Average Rating (1-5)'}\n",
    "    )\n",
    "    plt.title('Average Rating Heatmap (Year x Month)', fontsize=14)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Year')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "5d128b57de979922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Review Count by Year",
   "id": "dc75fca9620b5b86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure review_date and year\n",
    "if 'review_date' not in reviews_df.columns:\n",
    "    tmp = df_amazon_food_reviews.copy()\n",
    "    reviews_df = tmp.copy()\n",
    "    reviews_df['review_date'] = pd.to_datetime(reviews_df['Time'], errors='coerce', unit='s')\n",
    "if 'year' not in reviews_df.columns:\n",
    "    reviews_df['year'] = reviews_df['review_date'].dt.year\n",
    "\n",
    "valid = reviews_df.dropna(subset=['review_date']).copy()\n",
    "\n",
    "if valid.empty:\n",
    "    print('No review dates available to compute yearly counts.')\n",
    "else:\n",
    "    year_counts = valid.groupby('year').size().rename('review_count')\n",
    "\n",
    "    # Fill continuous year range with zeros\n",
    "    yr_min = int(valid['year'].min())\n",
    "    yr_max = int(valid['year'].max())\n",
    "    full_years = pd.Index(range(yr_min, yr_max + 1), name='year')\n",
    "    year_counts = year_counts.reindex(full_years, fill_value=0)\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "    sns.barplot(x=year_counts.index.astype(int), y=year_counts.values, color='#1f77b4')\n",
    "    plt.title('Review Count by Year', fontsize=14)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "5bb7fa62fa50e33d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.Review weight analysis",
   "id": "e99e579eff4d5802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.rcdefaults()\n",
    "\n",
    "# 0 Input & Preparation\n",
    "cols = ['ProductId', 'Score', 'HelpfulnessNumerator', 'HelpfulnessDenominator']\n",
    "reviews = df_amazon_food_reviews[cols].copy()\n",
    "\n",
    "# 1.1 Simple weight: pure Numerator\n",
    "reviews['weight_simple'] = reviews['HelpfulnessNumerator'].fillna(0).clip(lower=0)\n",
    "reviews.loc[reviews['weight_simple'] == 0, 'weight_simple'] = 1.0\n",
    "\n",
    "# 1.2 smoothed weight: Bayesian + log-scale Denominator\n",
    "a = b = 1.0\n",
    "r = (reviews['HelpfulnessNumerator'] + a) / (reviews['HelpfulnessDenominator'] + a + b)\n",
    "t = 1 + np.log1p(reviews['HelpfulnessDenominator'])\n",
    "reviews['weight_smooth'] = r * t\n",
    "\n",
    "# Clip weights to avoid extreme values\n",
    "reviews['weight_smooth'] = reviews['weight_smooth'].clip(lower=0.05, upper=10)"
   ],
   "id": "7deedc6a0c91e885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# main aggregation\n",
    "reviews['wscore_simple'] = reviews['Score'] * reviews['weight_simple']\n",
    "reviews['wscore_smooth'] = reviews['Score'] * reviews['weight_smooth']\n",
    "\n",
    "agg_main = (reviews\n",
    "    .groupby('ProductId', as_index=False)\n",
    "    .agg(\n",
    "        mean_score=('Score', 'mean'),               \n",
    "        num_reviews=('Score', 'size'),\n",
    "        sum_weight_simple=('weight_simple', 'sum'),\n",
    "        sum_wscore_simple=('wscore_simple', 'sum'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1) calculate weighted mean scores\n",
    "agg_main['weighted_mean_simple'] = np.where(\n",
    "    agg_main['sum_weight_simple'] > 0,\n",
    "    agg_main['sum_wscore_simple'] / agg_main['sum_weight_simple'],\n",
    "    np.nan\n",
    ")\n",
    "agg_main['weighted_mean_smooth'] = np.where(\n",
    "    agg_main['sum_weight_smooth'] > 0,\n",
    "    agg_main['sum_wscore_smooth'] / agg_main['sum_weight_smooth'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# 2) bayesian mean scores\n",
    "global_mean = reviews['Score'].mean()\n",
    "m = agg_main['sum_weight_smooth'].median()\n",
    "m = np.nan_to_num(m, nan=0.0)\n",
    "m = max(m, 1e-6)\n",
    "\n",
    "agg_main['bayesian_mean_smooth'] = (\n",
    "    (m * global_mean + agg_main['weighted_mean_smooth'] * agg_main['sum_weight_smooth']) /\n",
    "    (m + agg_main['sum_weight_smooth'])\n",
    ")\n",
    "\n",
    "m_simple = agg_main['sum_weight_simple'].median()\n",
    "m_simple = np.nan_to_num(m_simple, nan=0.0)\n",
    "m_simple = max(m_simple, 1e-6)\n",
    "\n",
    "agg_main['bayesian_mean_simple'] = (\n",
    "    (m_simple * global_mean + agg_main['weighted_mean_simple'] * agg_main['sum_weight_simple']) /\n",
    "    (m_simple + agg_main['sum_weight_simple'])\n",
    ")\n",
    "\n",
    "# 3) merge score table\n",
    "score_table = agg_main[['ProductId','mean_score','num_reviews',\n",
    "                        'weighted_mean_simple','weighted_mean_smooth',\n",
    "                        'bayesian_mean_smooth','bayesian_mean_simple']].copy()\n",
    "\n",
    "# 4) merge back to product info\n",
    "try:\n",
    "    df_amazon_product_info['product_id'] = df_amazon_product_info['product_id'].astype(str)\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    score_table['ProductId'] = score_table['ProductId'].astype(str)\n",
    "except Exception:\n",
    "    pass\n",
    "_metric_cols = ['mean_score','num_reviews','weighted_mean_simple','weighted_mean_smooth',\n",
    "                'bayesian_mean_smooth','bayesian_mean_simple']\n",
    "df_amazon_product_info = df_amazon_product_info.drop(columns=[c for c in _metric_cols if c in df_amazon_product_info.columns], errors='ignore')\n",
    "\n",
    "df_amazon_product_info = df_amazon_product_info.merge(\n",
    "    score_table, left_on='product_id', right_on='ProductId', how='left'\n",
    ")\n",
    "df_amazon_product_info = df_amazon_product_info.drop(columns=['ProductId'], errors='ignore')\n",
    "\n",
    "# 5) Applying ScoreDistribution\n",
    "from ast import literal_eval\n",
    "\n",
    "def _parse_score_distribution(x):\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return list(x)\n",
    "    if x is None:\n",
    "        return [0, 0, 0, 0, 0]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return [0, 0, 0, 0, 0]\n",
    "        try:\n",
    "            return list(literal_eval(s))\n",
    "        except Exception:\n",
    "            return [0, 0, 0, 0, 0]\n",
    "    # Catch NaN and other types\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return [0, 0, 0, 0, 0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [0, 0, 0, 0, 0]\n",
    "\n",
    "_sd = df_amazon_product_info['ScoreDistribution'].apply(_parse_score_distribution).apply(pd.Series)\n",
    "_sd.columns = ['count_1star','count_2star','count_3star','count_4star','count_5star']\n",
    "for c in _sd.columns:\n",
    "    df_amazon_product_info[c] = pd.to_numeric(_sd[c], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# 6) Additional columns\n",
    "for c in ['mean_score','weighted_mean_simple','weighted_mean_smooth','bayesian_mean_smooth','bayesian_mean_simple']:\n",
    "    if c in df_amazon_product_info.columns:\n",
    "        df_amazon_product_info[c] = pd.to_numeric(df_amazon_product_info[c], errors='coerce')\n",
    "\n",
    "df_amazon_product_info['diff_smooth_vs_simple'] = (\n",
    "    df_amazon_product_info['weighted_mean_smooth'] - df_amazon_product_info['weighted_mean_simple']\n",
    ")\n",
    "df_amazon_product_info['diff_bayes_vs_mean'] = (\n",
    "    df_amazon_product_info['bayesian_mean_smooth'] - df_amazon_product_info['mean_score']\n",
    ")\n",
    "\n",
    "# 7) Show sample\n",
    "cols_show = [\n",
    "    'product_id','num_reviews','mean_score',\n",
    "    'weighted_mean_simple','weighted_mean_smooth','bayesian_mean_smooth',\n",
    "    'diff_smooth_vs_simple','diff_bayes_vs_mean',\n",
    "    'count_1star','count_2star','count_3star','count_4star','count_5star'\n",
    "]\n",
    "df_amazon_product_info[cols_show].head(10)"
   ],
   "id": "330afc6447a2277"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# visualization\n",
    "req_cols = ['num_reviews','mean_score','weighted_mean_smooth','bayesian_mean_smooth']\n",
    "sample_base = df_amazon_product_info.dropna(subset=[c for c in req_cols if c in df_amazon_product_info.columns])\n",
    "sample = (sample_base\n",
    "          .query('num_reviews>=5')\n",
    "          .sample(min(300, len(sample_base)), random_state=42) if len(sample_base) > 0 else sample_base)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14,4), sharex=True, sharey=True)\n",
    "axes[0].scatter(sample['num_reviews'], sample['mean_score'], color='C0', alpha=0.6)\n",
    "axes[0].set_title('Mean')\n",
    "axes[1].scatter(sample['num_reviews'], sample['weighted_mean_smooth'], color='C1', alpha=0.6)\n",
    "axes[1].set_title('Weighted (smooth)')\n",
    "axes[2].scatter(sample['num_reviews'], sample['bayesian_mean_smooth'], color='C2', alpha=0.8)\n",
    "axes[2].set_title('Bayesian (smooth)')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('num_reviews')\n",
    "axes[0].set_ylabel('score')\n",
    "axes[0].set_ylim(1,5)\n",
    "plt.suptitle('Score vs. Reviews (3 separate charts)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# two examples\n",
    "one_review = df_amazon_product_info.query('num_reviews==1').head(1)\n",
    "sort_key = 'weighted_mean_smooth' if 'weighted_mean_smooth' in df_amazon_product_info.columns else 'num_reviews'\n",
    "many_reviews = (df_amazon_product_info.query('num_reviews>=20')\n",
    "                .sort_values(sort_key, ascending=False)\n",
    "                .head(1))\n",
    "display(one_review[cols_show[:6]])\n",
    "display(many_reviews[cols_show[:6]])"
   ],
   "id": "6bbadd71bb64acca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e2dc78e4c399b0c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Correlation Between Ratings and Product Descriptions\n",
    "\n",
    "### 4.1 Rating features\n",
    "1. Mean Score: The score displayed on the product detail page.\n",
    "    - Good Score: >= 4.5\n",
    "2. Score Polarization: Indicates whether the ratings for this product are polarized.\n",
    "    - Good Polarization: <= 0.25\n",
    "\n"
   ],
   "id": "c284227063e2acfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from correlation_with_description_analyse import *\n",
    "plt.rcdefaults()\n",
    "\n",
    "draw_product_score_distribution(df_amazon_product_info);"
   ],
   "id": "41a9b594d35c5d04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Visual Information Correlation\n",
    "\n",
    "#### 4.2.1 Correlation with the number of sample images\n",
    "\n",
    "- A statistical analysis was conducted on the relationship between the number of images in product descriptions and their ratings.\n",
    "- The line chart illustrates how the proportion of positive reviews and the proportion of low-polarity ratings change with the number of images."
   ],
   "id": "426f05d7bb187793"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "draw_correlation_with_n_images(df_amazon_product_info);",
   "id": "5d977de088618ba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Conclusion:** The number of images in product ratings and descriptions **<u>correlates positively</u>**. The more images included in the description, the greater the likelihood of the product receiving positive reviews.\n",
    "\n",
    "\n",
    "### 4.3 Text Information Correlation\n",
    "\n",
    "- **We use spearmanr to compute Correlation**\n",
    "- **We analyse text information in 4 aspects**\n",
    "    1. Correlation with description length\n",
    "    2. Correlation with reading ease\n",
    "    3. Correlation with the marketing tone of the description\n",
    "    4. Correlation with product description items\n",
    "\n",
    "#### 4.3.1 Correlation with description length\n"
   ],
   "id": "13943b4b4c466020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "draw_correlation_with_description_length(df_amazon_product_info);",
   "id": "29730e0fbd1d152b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Conclusion: There is <u>no significant correlation</u> between product ratings and the length of product descriptions.**\n",
    "\n",
    "\n",
    "\n",
    "#### 4.3.2 Correlation with reading ease\n",
    "\n",
    "**Types of reading ease:**\n",
    "1. **flesch reading ease**: The higher the score, the easier it is to read.\n",
    "2. **flesch kincaid grade**: The required grade level to read; The higher the number, the more difficult the reading level.\n",
    "3. **gunning fog**: Long word ratio; The higher the ratio, the harder it is to understand.\n"
   ],
   "id": "af2501ff77647023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, df = get_correlation_with_reading_ease(df_amazon_product_info);\n",
    "df"
   ],
   "id": "328dbe7991dbb991"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Conclusion: Product ratings show <u>no significant correlation</u> with reading difficulty.**\n",
    "\n",
    "#### 4.3.3 Correlation with the marketing tone of the description\n",
    "\n",
    "**Types of marketing tone:**\n",
    "1. **marketing tone score**: The proportion of exaggerated marketing language in product descriptions. The higher the score, the more exaggerated the marketing claims become.\n",
    "2. **sentiment score by language model**: model name is `distilbert-base-uncased-finetuned-sst-2-english`, The higher the score, the more positive the emotion.\n"
   ],
   "id": "85c268b81c407ca5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, df = get_correlation_with_marketing_sentiment(df_amazon_product_info)\n",
    "df"
   ],
   "id": "ccc65a9f03d82bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Conclusion:**\n",
    "- Product ratings show <u>no significant correlation</u> with marketing tone/sentiment.\n",
    "- However, since the metrics used to measure the emotional orientation of product descriptions do **not follow a normal distribution, <u>conclusions drawn from this basis may be unreliable</u>.**\n",
    "\n",
    "\n",
    "#### 4.3.4 Correlation with product description items\n",
    "\n",
    "> For this type of correlation where multiple labels correspond to a single value, regression models or correlation matrices could also be employed.\n",
    "> However, to maintain consistency with previous implementations, we still use the Spearman method here.\n"
   ],
   "id": "e2b0bf87fdb802de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, df = get_correlation_with_description_item(df_amazon_product_info)\n",
    "all_items = df['item'].unique()\n",
    "df = df[df.conclusion != \"No significant correlation\"].drop(columns=[\"category\", \"num_samples\"])\n",
    "correlated_items = df['item'].unique()\n",
    "print(f'{len(correlated_items)} of {len(all_items)} items are correlated with Score/ScorePolarization')\n",
    "df"
   ],
   "id": "5fa19f02fb04d2b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Conclusion:\n",
    "1. Products with the following description items might have a better rating:\n",
    "    - **Best Sellers Rank:** Only high-quality goods carry this label.\n",
    "    - **Item model number:** Products with model numbers may be more formal and reliable.\n",
    "    - **Product Dimensions:** Products with Product Dimensions may be more formal and reliable.\n",
    "    - **Directions:** Products accompanied by directions may reduce negative reviews caused by users' inability to use or misuse the product.\n",
    "    - **Safety Information:** Products with safety information may reduce negative reviews caused by user allergies.\n",
    "\n",
    "2. Products with the following description items might have a lower rating:\n",
    "    - **Package Dimensions:** Products with package dimensions may receive negative reviews due to logistics-related issues.\n",
    "\n",
    "#### 4.3.5 Conclusion for Product Descriptions\n",
    "\n",
    "|                   | For Sellers                                                                                                                | For Buyers                                                                                                                 |\n",
    "|-------------------|----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Image             | Increase the number of product images                                                                                      | Choose products with more pictures                                                                                         |\n",
    "| Descriptions Text | 1. Make product descriptions as professional as possible;<br>2. Provide more detailed safety information and instructions; | 1. Choose products that appear more professional;<br>2. Carefully read the product descriptions to avoid potential issues; |\n",
    "|                   |                                                                                                                            |                                                                                                                            |"
   ],
   "id": "c9b7aa8f1c1eecc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "931a09b8181e5d81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea00053f37595a27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff41f85afb2c27aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
